from typing import Any, Callable, Tuple

import jax
import jax.numpy as jnp
from hydrax.alg_base import Trajectory
from hydrax.algs.predictive_sampling import PredictiveSampling
from mujoco import mjx

from gpc.policy import Policy


class BootstrappedPredictiveSampling(PredictiveSampling):
    """Perform predictive sampling, but add samples from a generative policy."""

    def __init__(
        self,
        policy: Policy,
        observation_fn: Callable[[mjx.Data], jax.Array],
        num_policy_samples: int,
        **kwargs,
    ):
        """Initialize the controller.

        Args:
            policy: The generative policy to sample from.
            observation_fn: A function that produces an observation vector.
            num_policy_samples: The number of samples to take from the policy.
            **kwargs: Constructor arguments for PredictiveSampling.
        """
        self.observation_fn = observation_fn
        self.policy = policy
        self.num_policy_samples = num_policy_samples
        super().__init__(**kwargs)

    def optimize(self, state: mjx.Data, params: Any) -> Tuple[Any, Trajectory]:
        """Perform an optimization step to update the policy parameters.

        In addition to sampling random control sequences, also sample control
        sequences from the generative policy.

        Args:
            state: The initial state x₀.
            params: The current policy parameters, U ~ π(params).

        Returns:
            Updated policy parameters
            Rollouts used to update the parameters
        """
        rng, policy_rng, dr_rng = jax.random.split(params.rng, 3)

        # Sample random control sequences
        controls, params = self.sample_controls(params)
        controls = jnp.clip(controls, self.task.u_min, self.task.u_max)

        # Sample from the generative policy, which is conditioned on the latest
        # observation.
        y = self.observation_fn(state)
        prev = jnp.zeros((self.task.planning_horizon, self.task.model.nu))
        policy_rngs = jax.random.split(policy_rng, self.num_policy_samples)
        policy_controls = jax.vmap(self.policy.apply, in_axes=(None, None, 0))(
            prev, y, policy_rngs
        )

        # Combine the random and policy samples
        controls = jnp.concatenate([controls, policy_controls], axis=0)

        # Roll out the control sequences, applying domain randomizations and
        # combining costs using self.risk_strategy.
        rollouts = self.rollout_with_randomizations(state, controls, dr_rng)

        # Update the policy parameters based on the combined costs
        params = params.replace(rng=rng)
        params = self.update_params(params, rollouts)
        return params, rollouts
